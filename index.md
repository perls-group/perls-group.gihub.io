---
layout: default
---

Over the second half of 2022, the culture and context of AI development radically changed. Generative models for images and language are now common, for better and for worse. Elon Musk bought Twitter. FTX went bankrupt, prompting much soul-searching across the Effective Altruism community and some finger-pointing at AI Safety. Meanwhile, it turns out that RLHF (reinforcement learning from human feedback) actually works.

The first generation of AI ethics, focused on questions of short-term bias or long term existential risk, is ending. Instead, the next generation will strive to build good systems, whose capabilities and dynamics must be investigated rather than simply refused or made "provably beneficial".

In 2023, PERLS is relaunching with a reading group on recent critical papers, emerging topics, and guest presentations. The problem space will include both topics germane to RL, and more broadly to the patterns of decision-making that underpin the political economy of societal-scale feedback systems. In the spirit of PERLS, we will explore both technical and institutional aspects of this problem, uncovering new research questions and methods of investigation.

If interested, get in touch with Tom: thomaskrendlgilbert AT gmail.com
